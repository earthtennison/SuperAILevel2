{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictiveMaintainance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earthtennison/SuperAILevel2/blob/main/PredictiveMaintainance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iPt554ngBl3"
      },
      "source": [
        "<h1>Binary Classification</h1>\r\n",
        "<h6>Predict if that certain cycle time, found Abnormally</h6>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "source": [
        "import keras\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "import copy\r\n",
        "\r\n",
        "# Setting seed for reproducibility\r\n",
        "np.random.seed(1234)  \r\n",
        "PYTHONHASHSEED = 0\r\n",
        "\r\n",
        "from sklearn import preprocessing\r\n",
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score\r\n",
        "from keras.models import Sequential,load_model\r\n",
        "from keras.layers import Dense, Dropout, LSTM\r\n",
        "\r\n",
        "# define path to save model\r\n",
        "model_path = 'binary_model.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKLWmxWngx5y"
      },
      "source": [
        "#Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvE3wBGF3JLt"
      },
      "source": [
        "# !gdown --id xxxxxxxxxxxxxxxxxxxxxxxxxx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oka7xFL73woF"
      },
      "source": [
        "if not os.path.isfile(\"/content/xxxxxxxxxxxxxx.xlsx\"):\r\n",
        "  !unzip /content/xxxxxxxxxxxxx.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEcrhz2wvIUv"
      },
      "source": [
        "df = pd.read_excel('/content/xxxxxxxxxx.xlsx')\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5ZNbdGSMv7r"
      },
      "source": [
        "# !gdown --id xxxxxxxxxxxxxxxxxxxx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-72NJtxM0dr"
      },
      "source": [
        "df_chin = pd.read_csv('/content/xxxxxxxxxxxxxxxxxxx.csv')\r\n",
        "print(df_chin.groupby('Labels').count())\r\n",
        "\r\n",
        "df_chin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1uBcVAM08xy"
      },
      "source": [
        "#Data Preprocessing\r\n",
        "Abnormally date:<br>\r\n",
        "26/12/2019 15:13:00 PM<br>\r\n",
        "18/02/2020 10:35:35 AM\r\n",
        "\r\n",
        "Machine explode date (assume):<br>\r\n",
        "12/01/2020 15:00:00 PM\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd3Y_R3nRN-S"
      },
      "source": [
        "###explore data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyg78Uvb6Nk6"
      },
      "source": [
        "df_temp = df[['date','VI3108101X.PV','VI3108101Y.PV','VI3108102X.PV','VI3108102Y.PV','VI3108131X.PV','VI3108131Y.PV','VI3108132X.PV','VI3108132Y.PV','VI3108151X.PV','VI3108151Y.PV','VI3108152X.PV','VI3108152Y.PV','VI3108161X.PV','VI3108161Y.PV','VI3108162X.PV','VI3108162Y.PV']]\r\n",
        "\r\n",
        "df_temp1 = df_temp[\r\n",
        "(df['VI3108101X.PV'] >= 6) &\r\n",
        "(df['VI3108101Y.PV'] >= 6) &\r\n",
        "(df['VI3108102X.PV'] >= 6) &\r\n",
        "(df['VI3108102Y.PV'] >= 6) &\r\n",
        "(df['VI3108131X.PV'] >= 6) &\r\n",
        "(df['VI3108131Y.PV'] >= 6) &\r\n",
        "(df['VI3108132X.PV'] >= 6) &\r\n",
        "(df['VI3108132Y.PV'] >= 6) &\r\n",
        "(df['VI3108151X.PV'] >= 6) &\r\n",
        "(df['VI3108151Y.PV'] >= 6) &\r\n",
        "(df['VI3108152X.PV'] >= 6) &\r\n",
        "(df['VI3108152Y.PV'] >= 6) &\r\n",
        "(df['VI3108161X.PV'] >= 6) &\r\n",
        "(df['VI3108161Y.PV'] >= 6) &\r\n",
        "(df['VI3108162X.PV'] >= 6) &\r\n",
        "(df['VI3108162Y.PV'] >= 6)\r\n",
        "]\r\n",
        "df_temp.plot(x=\"date\", kind=\"line\", figsize=(20,8))\r\n",
        "df_temp1.plot(x=\"date\", kind=\"line\", figsize=(20,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdT9zHgORSDS"
      },
      "source": [
        "###chin's label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzLRLDXdM8J9"
      },
      "source": [
        "df['label_chin'] = df_chin['Labels']\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxTnEYKc8eIK"
      },
      "source": [
        "df =df[\r\n",
        "(df['VI3108101X.PV'] >= 6) &\r\n",
        "(df['VI3108101Y.PV'] >= 6) &\r\n",
        "(df['VI3108102X.PV'] >= 6) &\r\n",
        "(df['VI3108102Y.PV'] >= 6) &\r\n",
        "(df['VI3108131X.PV'] >= 6) &\r\n",
        "(df['VI3108131Y.PV'] >= 6) &\r\n",
        "(df['VI3108132X.PV'] >= 6) &\r\n",
        "(df['VI3108132Y.PV'] >= 6) &\r\n",
        "(df['VI3108151X.PV'] >= 6) &\r\n",
        "(df['VI3108151Y.PV'] >= 6) &\r\n",
        "(df['VI3108152X.PV'] >= 6) &\r\n",
        "(df['VI3108152Y.PV'] >= 6) &\r\n",
        "(df['VI3108161X.PV'] >= 6) &\r\n",
        "(df['VI3108161Y.PV'] >= 6) &\r\n",
        "(df['VI3108162X.PV'] >= 6) &\r\n",
        "(df['VI3108162Y.PV'] >= 6)\r\n",
        "]\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krd3js4FOc7c"
      },
      "source": [
        "df['label_chin']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_yOZvjLN8wE"
      },
      "source": [
        "###put all 4 abnormal to df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYVoZYsclGaY"
      },
      "source": [
        "#manual 1th\r\n",
        "start_man1 = '2018-01-01 12:00:00'\r\n",
        "detect_man1 = '2018-08-06 09:00:00'\r\n",
        "end_man1 = '2018-08-22 11:00:00'\r\n",
        "\r\n",
        "#manual 2th\r\n",
        "start_man2 = '2018-08-23 12:00:00'\r\n",
        "detect_man2 = '2019-05-16 09:00:00'\r\n",
        "end_man2 = '2019-07-19 07:00:00'\r\n",
        "\r\n",
        "#abnormal 1th\r\n",
        "start_ab1 = '2019-07-19 08:00:00'\r\n",
        "detect_ab1 = '2019-12-26 15:00:00'\r\n",
        "end_ab1 = '2020-01-12 15:00:00'\r\n",
        "\r\n",
        "#abnormal 2th\r\n",
        "start_ab2 = '2020-01-14 00:00:00'\r\n",
        "detect_ab2 = '2020-02-18 10:00:00'\r\n",
        "end_ab2 = '2020-03-04 00:00:00'\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dgNoO_W0Y5C"
      },
      "source": [
        "### manual 1\r\n",
        "mask = (df['date'] > start_man1) & (df['date'] <= end_man1)\r\n",
        "manual_1 = df.loc[mask]\r\n",
        "\r\n",
        "mask_label = (manual_1['date'] > detect_man1) & (manual_1['date'] <= end_man1)\r\n",
        "label = mask_label.astype(int)\r\n",
        "\r\n",
        "manual_1['label']=label\r\n",
        "manual_1['id'] = 1\r\n",
        "manual_1['cycle'] = [i for i in range(manual_1.shape[0])]\r\n",
        "\r\n",
        "###manual 2\r\n",
        "mask = (df['date'] > start_man2) & (df['date'] <= end_man2)\r\n",
        "manual_2 = df.loc[mask]\r\n",
        "\r\n",
        "mask_label = (manual_2['date'] > detect_man2) & (manual_2['date'] <= end_man2)\r\n",
        "label_2 = mask_label.astype(int)\r\n",
        "\r\n",
        "manual_2['label']=label_2\r\n",
        "manual_2['id']=2\r\n",
        "manual_2['cycle'] =[i for i in range(manual_2.shape[0])]\r\n",
        "\r\n",
        "###abnorm 1\r\n",
        "mask = (df['date'] > start_ab1) & (df['date'] <= end_ab1)\r\n",
        "abnorm_1 = df.loc[mask]\r\n",
        "\r\n",
        "mask_label = (abnorm_1['date'] > detect_ab1) & (abnorm_1['date'] <= end_ab1)\r\n",
        "label_1 = mask_label.astype(int)\r\n",
        "\r\n",
        "abnorm_1['label']=label_1\r\n",
        "abnorm_1['id'] = 3\r\n",
        "abnorm_1['cycle'] = [i for i in range(abnorm_1.shape[0])]\r\n",
        "\r\n",
        "###abnorm 2\r\n",
        "mask2 = (df['date'] > start_ab2) & (df['date'] <= end_ab2)\r\n",
        "abnorm_2 = df.loc[mask2]\r\n",
        "\r\n",
        "mask_label = (abnorm_2['date'] > detect_ab2) & (abnorm_2['date'] <= end_ab2)\r\n",
        "label_2 = mask_label.astype(int)\r\n",
        "\r\n",
        "abnorm_2['label']=label_2\r\n",
        "abnorm_2['id']=4\r\n",
        "abnorm_2['cycle'] =[i for i in range(abnorm_2.shape[0])]\r\n",
        "\r\n",
        "print(manual_1.shape)\r\n",
        "print(manual_2.shape)\r\n",
        "print(abnorm_1.shape)\r\n",
        "print(abnorm_2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4fNvvhVDUtK"
      },
      "source": [
        "df_new = pd.concat([manual_1,manual_2,abnorm_1,abnorm_2],axis=0)\r\n",
        "df_new.index = [i for i in range(df_new.shape[0])]\r\n",
        "df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXymrD8dOKxD"
      },
      "source": [
        "df_new.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11VeWAbVD0km"
      },
      "source": [
        "df_new = df_new[[\r\n",
        "       'date','id', 'cycle', 'FIC3104801.OP', 'FIC3104801.PV', 'FIC3104901.OP',\r\n",
        "       'FIC3104901.PV', 'FIC3105201.OP', 'FIC3105201.PV', 'JI3104901.PV',\r\n",
        "       'LI3104702.PV', 'LIC3105201.OP', 'LIC3105201.PV', 'LIC3105204.OP',\r\n",
        "       'LIC3105204.PV', 'PI3100508.PV', 'PI3104801.PV', 'PI3104803.PV',\r\n",
        "       'PI3104903.PV', 'PI3104905.PV', 'PIC3104701.OP', 'PIC3104701.PV',\r\n",
        "       'PIC3104901.OP', 'SI3104901.PV', 'TI3104801.PV', 'TI3104802.PV',\r\n",
        "       'TI3104901.PV', 'TI3104902.PV', 'TI3105203.PV', 'TI3105301.PV',\r\n",
        "       'TI3108102.PV', 'TI3108104.PV', 'TI3108105.PV', 'TI3108106.PV',\r\n",
        "       'TI3108107.PV', 'TI3108108.PV', 'TI3108111.PV', 'TI3108113.PV',\r\n",
        "       'TI3108114.PV', 'TI3108115.PV', 'TI3108116.PV', 'TI3108119.PV',\r\n",
        "       'TI3108131.PV', 'TI3108132.PV', 'TI3108134.PV', 'TI3108135.PV',\r\n",
        "       'TI3108136.PV', 'VI3108101X.PV', 'VI3108101Y.PV', 'VI3108102X.PV',\r\n",
        "       'VI3108102Y.PV', 'VI3108131X.PV', 'VI3108131Y.PV', 'VI3108132X.PV',\r\n",
        "       'VI3108132Y.PV', 'VI3108151X.PV', 'VI3108151Y.PV', 'VI3108152X.PV',\r\n",
        "       'VI3108152Y.PV', 'VI3108161X.PV', 'VI3108161Y.PV', 'VI3108162X.PV',\r\n",
        "       'VI3108162Y.PV', 'ZI3108105A.PV', 'ZI3108105B.PV', 'ZI3108155A.PV',\r\n",
        "       'ZI3108155B.PV', 'label','label_chin' \r\n",
        "]]\r\n",
        "df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrcEVqRQQ3NP"
      },
      "source": [
        "###RUL (remaining useful life)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRXPMGpxQlsC"
      },
      "source": [
        "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\r\n",
        "rul = pd.DataFrame(df_new.groupby('id')['cycle'].max()).reset_index()\r\n",
        "rul.columns = ['id', 'max']\r\n",
        "df_new = df_new.merge(rul, on=['id'], how='left')\r\n",
        "df_new['RUL'] = df_new['max'] - df_new['cycle'] #####important!!!\r\n",
        "df_new.drop('max', axis=1, inplace=True)\r\n",
        "df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcMaSwxhRkKZ"
      },
      "source": [
        "###Normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpIzQa3ARjka"
      },
      "source": [
        "# MinMax normalization (from 0 to 1)\r\n",
        "\r\n",
        "df_new['cycle_norm'] = df_new['cycle']\r\n",
        "cols_normalize = df_new.columns.difference(['date','id','cycle','RUL','label'])\r\n",
        "print(cols_normalize)\r\n",
        "\r\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\r\n",
        "norm_df = pd.DataFrame(min_max_scaler.fit_transform(df_new[cols_normalize]), \r\n",
        "                            columns=cols_normalize, \r\n",
        "                            index=df_new.index)\r\n",
        "join_df = df_new[df_new.columns.difference(cols_normalize)].join(norm_df)\r\n",
        "df_new = join_df.reindex(columns = df_new.columns)\r\n",
        "\r\n",
        "df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikNdN8uWU2mW"
      },
      "source": [
        "###More modification\r\n",
        "* predicted 7 days before (use label of 7 day forward)\r\n",
        "* train separately each gang of sensors\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq0BhOwgXmUt"
      },
      "source": [
        "df_new['id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqwgOZEqQ_49"
      },
      "source": [
        "df_new_ab1 = df_new[(df_new['id']==1)|(df_new['id']==2)]\r\n",
        "df_new_ab1['label_6'] = df_new_ab1['label'][6:]\r\n",
        "df_new_ab1 = df_new_ab1.dropna().reset_index(drop=True)\r\n",
        "\r\n",
        "df_new_ab1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eU2WrXyR4a1"
      },
      "source": [
        "\r\n",
        "df_new_ab2 = df_new[(df_new['id']==3)|(df_new['id']==4)]\r\n",
        "df_new_ab2['label_6'] = df_new_ab2['label'][6:]\r\n",
        "df_new_ab2 = df_new_ab2.dropna().reset_index(drop=True)\r\n",
        "df_new_ab2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7St5gIWWHvu"
      },
      "source": [
        "#compressor gang\r\n",
        "compressor = [\r\n",
        "'TI3108102.PV'\r\n",
        ",'TI3108104.PV'\r\n",
        ",'TI3108105.PV'\r\n",
        ",'TI3108106.PV'\r\n",
        ",'TI3108107.PV'\r\n",
        ",'TI3108108.PV'\r\n",
        ",'TI3108116.PV'\r\n",
        ",'TI3108119.PV'\r\n",
        "]\r\n",
        "\r\n",
        "#motor left and right gang\r\n",
        "motor_wing = [\r\n",
        "'TI3108111.PV'\r\n",
        ",'TI3108113.PV'\r\n",
        ",'TI3108114.PV'\r\n",
        ",'TI3108115.PV'\r\n",
        ",'TI3108116.PV'\r\n",
        ",'TI3108119.PV'\r\n",
        ",'TI3108131.PV'\r\n",
        ",'TI3108132.PV'\r\n",
        "]\r\n",
        "\r\n",
        "#inside motor gang\r\n",
        "motor_in = [\r\n",
        "'TI3108134.PV'\r\n",
        ",'TI3108135.PV'\r\n",
        ",'TI3108136.PV'\r\n",
        "\r\n",
        "]\r\n",
        "\r\n",
        "topTen = [\r\n",
        "'LIC3105201.PV'\r\n",
        ",'VI3108132Y.PV'\r\n",
        ",'LIC3105204.PV'\r\n",
        ",'FIC3105201.OP'\r\n",
        ",'TI3108114.PV'\r\n",
        ",'TI3105301.PV'\r\n",
        ",'TI3108115.PV'\r\n",
        ",'LIC3105204.OP'\r\n",
        ",'VI3108161X.PV'\r\n",
        ",'VI3108101Y.PV'\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlonnz5CXP__"
      },
      "source": [
        "df_com_1 = df_new_ab1[['date','id','cycle','RUL','cycle_norm']+compressor+['label','label_6']]\r\n",
        "df_motor_wing_1 = df_new_ab1[['date','id','cycle','RUL','cycle_norm']+motor_wing+['label','label_6']]\r\n",
        "df_motor_in_1 = df_new_ab1[['date','id','cycle','RUL','cycle_norm']+motor_in+['label','label_6']]\r\n",
        "df_topTen_1 = df_new_ab1[['date','id','cycle','RUL','cycle_norm']+topTen+['label','label_6']]\r\n",
        "\r\n",
        "df_com_2 = df_new_ab2[['date','id','cycle','RUL','cycle_norm']+compressor+['label','label_6']]\r\n",
        "df_motor_wing_2 = df_new_ab2[['date','id','cycle','RUL','cycle_norm']+motor_wing+['label','label_6']]\r\n",
        "df_motor_in_2 = df_new_ab2[['date','id','cycle','RUL','cycle_norm']+motor_in+['label','label_6']]\r\n",
        "df_topTen_2 = df_new_ab2[['date','id','cycle','RUL','cycle_norm']+topTen+['label','label_6']]\r\n",
        "df_topTen_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl12ZBV4PwLS"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEh_Ix0tt8ls"
      },
      "source": [
        "###slicing window"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bB51xcwOnq9"
      },
      "source": [
        "# pick a large window size of 50 cycles\r\n",
        "sequence_length = 50\r\n",
        "\r\n",
        "def input(df_temp,feature):\r\n",
        "  \r\n",
        "  # function to reshape features into (samples, time steps, features) \r\n",
        "  def gen_sequence(id_df, seq_length, seq_cols):\r\n",
        "      \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\r\n",
        "      we need to drop those which are below the window-length. An alternative would be to pad sequences so that\r\n",
        "      we can use shorter ones \"\"\"\r\n",
        "      # for one id I put all the rows in a single matrix\r\n",
        "      data_matrix = id_df[seq_cols].values\r\n",
        "      num_elements = data_matrix.shape[0]\r\n",
        "      # Iterate over two lists in parallel.\r\n",
        "      # For example id1 have 192 rows and sequence_length is equal to 50\r\n",
        "      # so zip iterate over two following list of numbers (0,142),(50,192)\r\n",
        "      # 0 50 -> from row 0 to row 50\r\n",
        "      # 1 51 -> from row 1 to row 51\r\n",
        "      # 2 52 -> from row 2 to row 52\r\n",
        "      # ...\r\n",
        "      # 141 191 -> from row 141 to 191\r\n",
        "      for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\r\n",
        "          yield data_matrix[start:stop, :]\r\n",
        "          \r\n",
        "  # pick the feature columns \r\n",
        "  # sequence_cols = [\r\n",
        "  #       'FIC3104801.OP', 'FIC3104801.PV', 'FIC3104901.OP',\r\n",
        "  #       'FIC3104901.PV', 'FIC3105201.OP', 'FIC3105201.PV', 'JI3104901.PV',\r\n",
        "  #       'LI3104702.PV', 'LIC3105201.OP', 'LIC3105201.PV', 'LIC3105204.OP',\r\n",
        "  #       'LIC3105204.PV', 'PI3100508.PV', 'PI3104801.PV', 'PI3104803.PV',\r\n",
        "  #       'PI3104903.PV', 'PI3104905.PV', 'PIC3104701.OP', 'PIC3104701.PV',\r\n",
        "  #       'PIC3104901.OP', 'SI3104901.PV', 'TI3104801.PV', 'TI3104802.PV',\r\n",
        "  #       'TI3104901.PV', 'TI3104902.PV', 'TI3105203.PV', 'TI3105301.PV',\r\n",
        "  #       'TI3108102.PV', 'TI3108104.PV', 'TI3108105.PV', 'TI3108106.PV',\r\n",
        "  #       'TI3108107.PV', 'TI3108108.PV', 'TI3108111.PV', 'TI3108113.PV',\r\n",
        "  #       'TI3108114.PV', 'TI3108115.PV', 'TI3108116.PV', 'TI3108119.PV',\r\n",
        "  #       'TI3108131.PV', 'TI3108132.PV', 'TI3108134.PV', 'TI3108135.PV',\r\n",
        "  #       'TI3108136.PV', 'VI3108101X.PV', 'VI3108101Y.PV', 'VI3108102X.PV',\r\n",
        "  #       'VI3108102Y.PV', 'VI3108131X.PV', 'VI3108131Y.PV', 'VI3108132X.PV',\r\n",
        "  #       'VI3108132Y.PV', 'VI3108151X.PV', 'VI3108151Y.PV', 'VI3108152X.PV',\r\n",
        "  #       'VI3108152Y.PV', 'VI3108161X.PV', 'VI3108161Y.PV', 'VI3108162X.PV',\r\n",
        "  #       'VI3108162Y.PV', 'ZI3108105A.PV', 'ZI3108105B.PV', 'ZI3108155A.PV',\r\n",
        "  #       'ZI3108155B.PV','cycle_norm'\r\n",
        "                \r\n",
        "  # ]\r\n",
        "\r\n",
        "\r\n",
        "  sequence_cols = feature\r\n",
        "\r\n",
        "  # generator for the sequences\r\n",
        "  seq_gen = (list(gen_sequence(df_temp[df_temp['id']==id], sequence_length, sequence_cols)) \r\n",
        "            for id in df_temp['id'].unique())\r\n",
        "  # generate sequences and convert to numpy array\r\n",
        "  seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\r\n",
        "  print(\"sequence\")\r\n",
        "  print(seq_array.shape)\r\n",
        "  print(seq_array)\r\n",
        "\r\n",
        "\r\n",
        "  # function to generate labels\r\n",
        "  def gen_labels(id_df, seq_length, label):\r\n",
        "      # For one id I put all the labels in a single matrix.\r\n",
        "      # For example:\r\n",
        "      # [[1]\r\n",
        "      # [4]\r\n",
        "      # [1]\r\n",
        "      # [5]\r\n",
        "      # [9]\r\n",
        "      # ...\r\n",
        "      # [200]] \r\n",
        "      data_matrix = id_df[label].values\r\n",
        "      num_elements = data_matrix.shape[0]\r\n",
        "      # I have to remove the first seq_length labels\r\n",
        "      # because for one id the first sequence of seq_length size have as target\r\n",
        "      # the last label (the previus ones are discarded).\r\n",
        "      # All the next id's sequences will have associated step by step one label as target. \r\n",
        "      return data_matrix[seq_length:num_elements, :]\r\n",
        "\r\n",
        "  # generate labels\r\n",
        "\r\n",
        "  lab = ['label_6']\r\n",
        "  label_gen = [gen_labels(df_temp[df_temp['id']== id], sequence_length, lab) for id in df_temp['id'].unique()]\r\n",
        "  label_array = (np.concatenate(label_gen).astype(np.float32))\r\n",
        "  # print(\"label\")\r\n",
        "  # print(label_array)\r\n",
        "  # print(label_array.shape)\r\n",
        "\r\n",
        "  return seq_array,label_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50kry4GQYovX"
      },
      "source": [
        "###change input feature here!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojYi-KdTTVfJ"
      },
      "source": [
        "#change both df and fea\r\n",
        "seq_array_ab1, label_array_ab1 = input(df_motor_wing_1, motor_wing)\r\n",
        "seq_array_ab2, label_array_ab2 = input(df_motor_wing_2, motor_wing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLt5AWnlvoqk"
      },
      "source": [
        "label_array_ab1 = np.array(label_array_ab1)\r\n",
        "print(label_array_ab1)\r\n",
        "print(len(label_array_ab1))\r\n",
        "print(label_array_ab1[0])\r\n",
        "print(len(label_array_ab1[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZUgkXdByGua"
      },
      "source": [
        "label_array_ab1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4MjplbUT8Yy"
      },
      "source": [
        "# Next, we build a deep network. \r\n",
        "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \r\n",
        "# Dropout is also applied after each LSTM layer to control overfitting. \r\n",
        "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\r\n",
        "# build the network\r\n",
        "def train(seq_array,label_array):\r\n",
        "\r\n",
        "  nb_features = seq_array.shape[2]\r\n",
        "  nb_out = label_array.shape[1]\r\n",
        "  sequence_length = 50\r\n",
        "\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  model.add(LSTM(\r\n",
        "          input_shape=(sequence_length, nb_features),\r\n",
        "          units=100,\r\n",
        "          return_sequences=True))\r\n",
        "  model.add(Dropout(0.2))\r\n",
        "\r\n",
        "  model.add(LSTM(\r\n",
        "            units=50,\r\n",
        "            return_sequences=False))\r\n",
        "  model.add(Dropout(0.2))\r\n",
        "\r\n",
        "  model.add(Dense(units=nb_out, activation='sigmoid'))\r\n",
        "#   model.add(Dense(units=2, activation='softmax'))\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "  print(model.summary())\r\n",
        "\r\n",
        "  # fit the network\r\n",
        "  history = model.fit(seq_array, label_array, epochs=50, batch_size=200, validation_split=0.05, verbose=2,\r\n",
        "            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\r\n",
        "                        # keras.callbacks.ModelCheckpoint(filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\", monitor='val_loss', save_weights_only=True, save_best_only=True, mode='min', verbose=1)\r\n",
        "                        keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)\r\n",
        "                        ]\r\n",
        "            )\r\n",
        "  model.save('best_model.h5')\r\n",
        "\r\n",
        "  # list all data in history\r\n",
        "  print(history.history.keys())\r\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-rAolORanak"
      },
      "source": [
        "###change train input here!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sH4lEUHamvI"
      },
      "source": [
        "model, history = train(seq_array_ab1,label_array_ab1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYm_Ty6OUcYq"
      },
      "source": [
        "#Evaluation val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTWkXfXrUTTk"
      },
      "source": [
        "# summarize history for Accuracy\r\n",
        "fig_acc = plt.figure(figsize=(10, 10))\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "fig_acc.savefig(\"model_accuracy.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duPHZsRSUpuq"
      },
      "source": [
        "# summarize history for Loss\r\n",
        "fig_acc = plt.figure(figsize=(10, 10))\r\n",
        "plt.plot(history.history['loss'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'val'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "fig_acc.savefig(\"model_loss.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNiFLJszUgn9"
      },
      "source": [
        "###Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4zZP9pvUuwx"
      },
      "source": [
        "def predict(model, seq_array,label_array):\r\n",
        "  # training metrics\r\n",
        "\r\n",
        "  scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\r\n",
        "  print('Accurracy: {}'.format(scores[1]))\r\n",
        "\r\n",
        "  # make predictions and compute confusion matrix\r\n",
        "  y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\r\n",
        "  y_true = label_array\r\n",
        "\r\n",
        "  temp = pd.DataFrame(y_pred)\r\n",
        "  temp.to_csv('binary_submit_train.csv', index = None)\r\n",
        "\r\n",
        "  print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\r\n",
        "  cm = confusion_matrix(y_true, y_pred)\r\n",
        "  print(cm)\r\n",
        "\r\n",
        "  # compute precision and recall\r\n",
        "  precision = precision_score(y_true, y_pred)\r\n",
        "  recall = recall_score(y_true, y_pred)\r\n",
        "  print( 'precision = ', precision, '\\n', 'recall = ', recall)\r\n",
        "  print('\\n')\r\n",
        "  # Plot in red color the predicted data and in green color the\r\n",
        "  # actual data to verify visually the accuracy of the model.\r\n",
        "  fig_verify = plt.figure(figsize=(10, 5))\r\n",
        "  plt.plot(y_pred, color=\"red\")\r\n",
        "  plt.plot(y_true, color=\"green\")\r\n",
        "  plt.title('prediction')\r\n",
        "  plt.ylabel('value')\r\n",
        "  plt.xlabel('row')\r\n",
        "  plt.legend(['predicted', 'actual data'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  fig_verify.savefig(\"model_verify.png\")\r\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNNPujp-bkCO"
      },
      "source": [
        "###change predict input here!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBF6NiRZVrIw"
      },
      "source": [
        "prediction = predict(model, seq_array_ab2, label_array_ab2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMKxl1MO36om"
      },
      "source": [
        "#Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-budE0ZpU_e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwZ5UmDu2zON"
      },
      "source": [
        "df_test = df #excel from BI\r\n",
        "df_test['id']=1\r\n",
        "\r\n",
        "df_test = df_test[\r\n",
        "(df['VI3108101X.PV'] >= 6) &\r\n",
        "(df['VI3108101Y.PV'] >= 6) &\r\n",
        "(df['VI3108102X.PV'] >= 6) &\r\n",
        "(df['VI3108102Y.PV'] >= 6) &\r\n",
        "(df['VI3108131X.PV'] >= 6) &\r\n",
        "(df['VI3108131Y.PV'] >= 6) &\r\n",
        "(df['VI3108132X.PV'] >= 6) &\r\n",
        "(df['VI3108132Y.PV'] >= 6) &\r\n",
        "(df['VI3108151X.PV'] >= 6) &\r\n",
        "(df['VI3108151Y.PV'] >= 6) &\r\n",
        "(df['VI3108152X.PV'] >= 6) &\r\n",
        "(df['VI3108152Y.PV'] >= 6) &\r\n",
        "(df['VI3108161X.PV'] >= 6) &\r\n",
        "(df['VI3108161Y.PV'] >= 6) &\r\n",
        "(df['VI3108162X.PV'] >= 6) &\r\n",
        "(df['VI3108162Y.PV'] >= 6)\r\n",
        "]\r\n",
        "df_test = df_test[['date','id']+motor_wing]\r\n",
        "\r\n",
        "# mask = (df_test['date'] > '2019-07-19 04:00:00\t') & (df_test['date'] <= '2020-03-04 23:00:00')\r\n",
        "# df_test = df_test.loc[mask]\r\n",
        "\r\n",
        "df_test[['date']+motor_wing].plot(x=\"date\", kind=\"line\", figsize=(20,8))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBz5Ci_QhwAn"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41VRq0qe3Bo-"
      },
      "source": [
        "def pipeline(df_test):\r\n",
        "\r\n",
        "  \r\n",
        "  #preprocess\r\n",
        "\r\n",
        "  # MinMax normalization (from 0 to 1)\r\n",
        "\r\n",
        "  cols_normalize = df_test.columns.difference(['date','id'])\r\n",
        "  print(cols_normalize)\r\n",
        "\r\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\r\n",
        "  norm_df = pd.DataFrame(min_max_scaler.fit_transform(df_test[cols_normalize]), \r\n",
        "                              columns=cols_normalize, \r\n",
        "                              index=df_test.index)\r\n",
        "  join_df = df_test[df_test.columns.difference(cols_normalize)].join(norm_df)\r\n",
        "  df_test = join_df.reindex(columns = df_test.columns)\r\n",
        "\r\n",
        "\r\n",
        "  # function to reshape features into (samples, time steps, features) \r\n",
        "  def gen_sequence(id_df, seq_length, seq_cols):\r\n",
        "      \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\r\n",
        "      we need to drop those which are below the window-length. An alternative would be to pad sequences so that\r\n",
        "      we can use shorter ones \"\"\"\r\n",
        "      # for one id I put all the rows in a single matrix\r\n",
        "      data_matrix = id_df[seq_cols].values\r\n",
        "      num_elements = data_matrix.shape[0]\r\n",
        "      # Iterate over two lists in parallel.\r\n",
        "      # For example id1 have 192 rows and sequence_length is equal to 50\r\n",
        "      # so zip iterate over two following list of numbers (0,142),(50,192)\r\n",
        "      # 0 50 -> from row 0 to row 50\r\n",
        "      # 1 51 -> from row 1 to row 51\r\n",
        "      # 2 52 -> from row 2 to row 52\r\n",
        "      # ...\r\n",
        "      # 141 191 -> from row 141 to 191\r\n",
        "      for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\r\n",
        "          yield data_matrix[start:stop, :]\r\n",
        "\r\n",
        "\r\n",
        "  sequence_cols = motor_wing\r\n",
        "  sequence_length = 50\r\n",
        "\r\n",
        "  # generator for the sequences\r\n",
        "  seq_gen = (list(gen_sequence(df_test[df_test['id']==id], sequence_length, sequence_cols)) \r\n",
        "            for id in df_test['id'].unique())\r\n",
        "  # generate sequences and convert to numpy array\r\n",
        "  seq_array_test = np.concatenate(list(seq_gen)).astype(np.float32)\r\n",
        "  print(\"sequence\")\r\n",
        "  print(seq_array_test.shape)\r\n",
        "  print(seq_array_test)\r\n",
        "\r\n",
        "  # if best iteration's model was saved then load and use it\r\n",
        "  model_path = '/content/best_model.h5'\r\n",
        "  if os.path.isfile(model_path):\r\n",
        "    model = load_model(model_path)\r\n",
        "\r\n",
        "  y_pred = model.predict_classes(seq_array_test,verbose=1, batch_size=200)\r\n",
        "\r\n",
        "  #save result\r\n",
        "  temp = pd.DataFrame(y_pred)\r\n",
        "  temp.to_csv('binary_submit_train.csv', index = None)\r\n",
        "\r\n",
        "  # Plot in red color the predicted data\r\n",
        "  # actual data to verify visually the accuracy of the model.\r\n",
        "  fig_verify = plt.figure(figsize=(10, 5))\r\n",
        "  plt.plot(y_pred, color=\"red\")\r\n",
        "  plt.title('prediction')\r\n",
        "  plt.ylabel('value')\r\n",
        "  plt.xlabel('row')\r\n",
        "  plt.legend(['predicted'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  fig_verify.savefig(\"model_verify.png\")\r\n",
        "\r\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX586jLAMdSl"
      },
      "source": [
        "prediction = pipeline(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO5YJ-DOoRgb"
      },
      "source": [
        "pred_list = []\r\n",
        "for x in prediction:\r\n",
        "  pred_list.append(x[0])\r\n",
        "len(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OODRWHIGldKM"
      },
      "source": [
        "pred_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIROfBWcvOoG"
      },
      "source": [
        "# def clean_pred(df_test):\r\n",
        "\r\n",
        "# #find first date of each id and delete first 50 row\r\n",
        "#   df_test = copy.deepcopy(df_test)\r\n",
        "\r\n",
        "#   for i in df_test['id'].unique(): #3,4\r\n",
        "\r\n",
        "#     temp = df_test[df_test['id']==i]\r\n",
        "#     # print(temp.index[0])\r\n",
        "\r\n",
        "#     index = [i for i in range(temp.index[0],temp.index[0]+50)]\r\n",
        "#     # print(index)\r\n",
        "\r\n",
        "#     df_test = df_test.drop(index)\r\n",
        "\r\n",
        "#   return df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq5jLPwSq3p2"
      },
      "source": [
        "# temp = df_new_ab2[df_new_ab2['id']==4]\r\n",
        "# temp2 =temp[temp['label_6']==1]\r\n",
        "# temp2['date'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr3xgtC1pf8L"
      },
      "source": [
        "# noti_date = []\r\n",
        "# for i in df_new_ab2['id'].unique():\r\n",
        "#   temp = df_new_ab2[df_new_ab2['id']==i]\r\n",
        "#   noti_date.append(temp['date'].iloc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL44pcnoo8Xf"
      },
      "source": [
        "pred_date = df_test[50:]#clean slicing window\r\n",
        "pred_date['predict'] = pred_list\r\n",
        "# pred_date = pred_date[pred_date['predict']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xcOkNyNpZWl"
      },
      "source": [
        "pred_date = pred_date[['date','predict']]\r\n",
        "pred_date[\"date\"] = pd.to_datetime(pred_date [\"date\"])\r\n",
        "pred_date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lHAnOLHnxJW"
      },
      "source": [
        "###check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STioOSPElqkU"
      },
      "source": [
        "mask = (pred_date['date'] > '2017-05-01 04:00:00\t') & (pred_date['date'] <= '2022-05-25 23:00:00')\r\n",
        "pred_date_temp = pred_date.loc[mask]\r\n",
        "\r\n",
        "fig_verify = plt.figure(figsize=(20, 5))\r\n",
        "pred_date_temp = pred_date_temp.set_index('date')\r\n",
        "lines = pred_date_temp['predict'].plot.line()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-7-k4fgp7UT"
      },
      "source": [
        "###Por's code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yync5f1p6bv"
      },
      "source": [
        "import datetime \r\n",
        "\r\n",
        "notify = list()\r\n",
        "for idx, row in pred_date.iterrows():\r\n",
        "    if row[\"predict\"] == 1:\r\n",
        "        notify.append(row[\"date\"])\r\n",
        "\r\n",
        "notify_before_seven_days = [notify[0].to_pydatetime()]\r\n",
        "out = list()\r\n",
        "out.append(notify[0].to_pydatetime())\r\n",
        "\r\n",
        "last_alert = notify[0].to_pydatetime()\r\n",
        "cnt = 0\r\n",
        "cnt_out = list()\r\n",
        "for n in notify:\r\n",
        "    if (n.to_pydatetime() - last_alert).total_seconds() > 86400:\r\n",
        "        out.append(n.to_pydatetime())\r\n",
        "        notify_before_seven_days.append(n.to_pydatetime() - datetime.timedelta(days=7))\r\n",
        "        last_alert = n.to_pydatetime()\r\n",
        "        cnt_out.append(cnt)\r\n",
        "        cnt = 0\r\n",
        "    else:\r\n",
        "        cnt += 1\r\n",
        "        last_alert = n.to_pydatetime()\r\n",
        "cnt_out.append(cnt)\r\n",
        "\r\n",
        "date_final = notify_before_seven_days[np.argmax(cnt_out)].strftime(\"%m/%d/%Y %H:%M:%S\")\r\n",
        "date_final2 = out[np.argmax(cnt_out)].strftime(\"%m/%d/%Y %H:%M:%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQ9S7xurtwb"
      },
      "source": [
        "date_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C1lb1ljvsZb"
      },
      "source": [
        "print('The predicted date for anomaly is')\r\n",
        "print(date_final)\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "print('The notify date for anomaly is')\r\n",
        "print(date_final2)\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "print('Sensors cause')\r\n",
        "print(motor_wing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5fRWmRTw2wQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQvxxizsjEo-"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF8TYOeejnvV"
      },
      "source": [
        "df_new_ab2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi-K7JKrjE8C"
      },
      "source": [
        "\r\n",
        "\r\n",
        "#preprocess\r\n",
        "# function to reshape features into (samples, time steps, features) \r\n",
        "def gen_sequence(id_df, seq_length, seq_cols):\r\n",
        "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\r\n",
        "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\r\n",
        "    we can use shorter ones \"\"\"\r\n",
        "    # for one id I put all the rows in a single matrix\r\n",
        "    data_matrix = id_df[seq_cols].values\r\n",
        "    num_elements = data_matrix.shape[0]\r\n",
        "    # Iterate over two lists in parallel.\r\n",
        "    # For example id1 have 192 rows and sequence_length is equal to 50\r\n",
        "    # so zip iterate over two following list of numbers (0,142),(50,192)\r\n",
        "    # 0 50 -> from row 0 to row 50\r\n",
        "    # 1 51 -> from row 1 to row 51\r\n",
        "    # 2 52 -> from row 2 to row 52\r\n",
        "    # ...\r\n",
        "    # 141 191 -> from row 141 to 191\r\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\r\n",
        "        yield data_matrix[start:stop, :]\r\n",
        "\r\n",
        "\r\n",
        "sequence_cols = motor_wing\r\n",
        "sequence_length = 50\r\n",
        "\r\n",
        "# generator for the sequences\r\n",
        "seq_gen = (list(gen_sequence(df_test[df_test['id']==id], sequence_length, sequence_cols)) \r\n",
        "          for id in df_test['id'].unique())\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# generate sequences and convert to numpy array\r\n",
        "seq_array_test = np.concatenate(list(seq_gen)).astype(np.float32)\r\n",
        "print(\"sequence\")\r\n",
        "print(seq_array_test.shape)\r\n",
        "print(seq_array_test)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# if best iteration's model was saved then load and use it\r\n",
        "model_path = '/content/best_model.h5'\r\n",
        "if os.path.isfile(model_path):\r\n",
        "  model = load_model(model_path)\r\n",
        "\r\n",
        "y_pred = model.predict_classes(seq_array_test,verbose=1, batch_size=200)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#save result\r\n",
        "temp = pd.DataFrame(y_pred)\r\n",
        "temp.to_csv('binary_submit_train.csv', index = None)\r\n",
        "\r\n",
        "# Plot in red color the predicted data\r\n",
        "# actual data to verify visually the accuracy of the model.\r\n",
        "fig_verify = plt.figure(figsize=(10, 5))\r\n",
        "plt.plot(y_pred, color=\"red\")\r\n",
        "plt.title('prediction')\r\n",
        "plt.ylabel('value')\r\n",
        "plt.xlabel('row')\r\n",
        "plt.legend(['predicted'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "fig_verify.savefig(\"model_verify.png\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}